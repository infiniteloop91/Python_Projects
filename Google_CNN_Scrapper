#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""

@author: ericlyons
"""

import requests
import sys
import webbrowser
import bs4
import pandas as pd
import time


page_list = [1,11,21,31,41,51,61,71,81,91]
item = 0
urls = []
abstracts = [] 
titles = []
section = []
date = []
df = pd.DataFrame()
api_key = 'XXXXXXXXXXXX'
from apiclient.discovery import build
for i in range(len(page_list)):
    try:
        pages = page_list[i]
        resource = build("customsearch", 'v1', developerKey=api_key).cse()
        results = resource.list(q='hurricane maria', start = pages,  cx='XXXXXXXXXXXXXXXXX').execute()
        time.sleep(5)
        print(i)
        number = 0
       
            
        while number <= 20000: 
          titles.append(results['items'][number]['title'])
          urls.append(results['items'][number]['link'])
          abstracts.append(results['items'][number]['snippet'])
          number = number + 1

                
        
    
    #error handling 
    except KeyError:
         #print('Key error \n')
         pass
    except TypeError:
         #print('Key error \n')
         pass
    except IndexError:
        #print('Index error \n')
        pass


df['urls'] = urls
df['abstracts'] = abstracts
df['title'] = titles
df['section'] = section
df['dates'] = date

export_csv = df.to_csv('/Users/ericlyons/Desktop/CNN.csv', index = None, header=True) 

